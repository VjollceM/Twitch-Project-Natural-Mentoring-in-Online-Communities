{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "#tokenization \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#normalization\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "\n",
    "logger = logging.getLogger (__name__)\n",
    "\n",
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer (morphology)\n",
    "extractor = TurkishSentenceExtractor()\n",
    "\n",
    "#lemmatization\n",
    "\n",
    "import zeyrek\n",
    "\n",
    "#compute ngrams\n",
    "from gensim.models import Phrases\n",
    "\n",
    "#generate a wordcloud image\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing libraries for lda\n",
    "\n",
    "import gensim\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "#visualize the LDA model\n",
    "import matplotlib.pyplot as plt \n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b4102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "\n",
    "data = pd.read_csv(\"mydata.csv\", error_bad_lines=False, engine ='python')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd6a407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42890"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d094ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user_description'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "#storing the puntuation free text\n",
    "data['clean']= data['text'].apply(lambda x:remove_punctuation(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase the text\n",
    "\n",
    "data['lower']= data['clean'].apply(lambda x: x.lower())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba09f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing text\n",
    "\n",
    "nltk.word_tokenize('lower', language='turkish')\n",
    "data['textokenized'] = data.apply(lambda row: nltk.word_tokenize(row['lower']), axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "\n",
    "nltk.download ('stopwords')\n",
    "#Stop words present in the library\n",
    "stops = nltk.corpus.stopwords.words('turkish')\n",
    "newstops = ['sa','as', 'ben', 'bende', 'imühamkalp', '2', 'hayır', 'mod', 'cisillll', 'pquraen', 'al', 'mis', 'bent', 'imuhamopustuk', 'senin', 'sen', 'onu', 'bence', 'darkholmee', 'x', 'mu', 'ay', 'naber', 'imuhamiys', 'olur', 'selam', 'insta', 'httpswwwinstagramcomimuhammetyildizhltr', 'haydaaaa', 'hellllooo', 'hellooo', 'hellooooo', 'heee', 'şey', 'haydaaaaaaa', 'haydaaaaaaaa', 'am', 'da', 'daha', 'iyi', 'napalion7', 'bi', 'bosver', 'imuhamalo', 'napalion', 'merhaba', 'httpswwwyoutubecomchannelucstjlfccjuguahupwssiww', 'tutodvrm', 'lul', 'on',  'slm', 'aa', 'şş', 'aaa', 'aaaa', 'aaaaaa', 'aaaaaaa', 'mamiş', 'D', 'çok', 'az', 'neyse', 'falcon2kkalp', 'httpsdiscordggrdj7hxmvgy', ' falcon2kkalp', 'the', 'idil', 'bu', 'httpsdiscordggrdj7hxmvgy', 'neyse', 'ya', 'ne', 'nympea', 'at', 'ays', 'burcinvr', 'san', 'sela', 'değil', 'tutodurm', 'ah', 'be', 'kerema5keremkalp','sö', 'falan', 'bc', 'dc', 'he', 'up', 'aq', 'fizy', '10', 'su', 'nim', 'efe', 'hee', 'hb', 'a', 'gsyi', 'ad', 'ha', 'ner', 'berra', 'hg', 'cafercan3125', 'ayn', 'amk', 'do', 'fa', 'alt','var', 'ba', 'ronaldo', 'ara', 'ada', 'mami', 'bura', 'on', 'fa', 'bb', 'ge', 'ab', 'btw', 'mk', 'pek','Kerem','Keremakdemirr', 'keremakdemirr', 'kere', 'ays055', 'burçin', 'imühamiys', 'mi', 'aç', 'httpswwwyoutubecomchanneluçstjlfçcjuguahupwssiww', 'nympeadansdans','kerem', 'httpsyoutubevsr46l41bru', 'httpswwwinstagramcomkeremakdemirr', 'cafer', 'cüneyt','e', 'i', '5', 'lan', 'la', 'se', 'd', '3', '1', 'bir', 'cafer', 'yay', 'imuhammetyildiz', 'muhamed','deniz', 'Deniz', 'nympea', 'imuhaiys', 'alo', 'evet', 'hayir', 'yok', 'oha']\n",
    "stops.extend(newstops)\n",
    "\n",
    "data['nostopwords'] = data['textokenized'].apply(\n",
    "    lambda x: [word for word in x if word not in stops])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5fbc65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-03 22:38:22,599 - __main__ - INFO\n",
      "Msg: Sentences normalized in: 1102.4149286746979 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  normalization\n",
    "\n",
    "def normalize_long_text(text):\n",
    "    normalized_sentences = [normalizer.normalize(word) for word in text]\n",
    "    normalized_text = \" \".join(normalized_sentences)\n",
    "    return normalized_text\n",
    "\n",
    "sentences = data['nostopwords'].copy()\n",
    "new_sent = []\n",
    "start = time.time()\n",
    "\n",
    "for token in sentences:   \n",
    "    if token.count('') > 0:\n",
    "        token = list(filter(('').__ne__, token))\n",
    "    new_token = normalize_long_text(token)\n",
    "    new_sent.append(new_token)\n",
    "\n",
    "logger.info(f\"Sentences normalized in: {time.time() - start} s\")\n",
    "\n",
    "splitted_words = []\n",
    "for sent in new_sent:\n",
    "    words = sent.split()\n",
    "    splitted_words.append(words)\n",
    "    \n",
    "for token in splitted_words:\n",
    "    j = 0\n",
    "    for word in token:\n",
    "        new_word = word.replace('\"', '').replace(\"’\", '').replace(\"'\", '').replace(\"”\", '')\n",
    "        token[j] = new_word\n",
    "        j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "lem_sent = []\n",
    "for sent in splitted_words:\n",
    "    normalized_sent = []\n",
    "    for word in sent:\n",
    "        if word == '':\n",
    "            continue\n",
    "        else:\n",
    "            lem_word = analyzer.lemmatize(word)\n",
    "            normalized_sent.append(lem_word[0][1][0])\n",
    "    lem_sent.append(normalized_sent)\n",
    "    \n",
    "x = lem_sent.copy()\n",
    "for sent in x:\n",
    "    i = 0\n",
    "    for token in sent:\n",
    "        sent[i] = token.lower()\n",
    "        i += 1\n",
    "lem_sent = x\n",
    "\n",
    "\n",
    "lem_sent = list(filter(('').__ne__, lem_sent))\n",
    "data['lemmatized'] = lem_sent\n",
    "data['lemma_str'] = [' '.join(map(str, l)) for l in data['lemmatized']]\n",
    "\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d23ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "\n",
    "bigram = Phrases(data.lemmatized, min_count=20)\n",
    "for idx in range(len(data.lemmatized)):\n",
    "    for token in bigram[data.lemmatized[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            data.lemmatized[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords again after lemmatization\n",
    "\n",
    "data['final_text'] = data['lemmatized'].apply(\n",
    "    lambda x: [word for word in x if word not in stops])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "243d2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the dataframe to csv\n",
    "data.to_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a289cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stops, background_color=\"white\", width=800, height=400).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.figure( figsize=(40,20))\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d0179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA \n",
    "\n",
    "# Create Dictionary \n",
    "id2word = corpora.Dictionary(data['final_text'])  \n",
    "# Create Corpus \n",
    "texts = data['final_text']  \n",
    "# Term Document Frequency \n",
    "corpus = [id2word.doc2bow(text) for text in texts]  \n",
    "# View \n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ade26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keyword of topics- LDA\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence\n",
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "     coherence_model_lda = CoherenceModel(model=lda_model, texts= data ['final_text'], \n",
    "                                                          dictionary=id2word, \n",
    "                                                              coherence='u_mass')\n",
    "     coherence_lda = coherence_model_lda.get_coherence()\n",
    "     print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03e9b1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1612821714458870406209561259\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1612821714458870406209561259_data = {\"mdsDat\": {\"x\": [0.11432334837929974, -0.034316385333048836, -0.03383837621683549, -0.0461685868294155], \"y\": [0.002982173046297094, -0.08095190587471204, 0.038997190860950624, 0.03897254196746431], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [35.14290393335339, 23.641069261562762, 22.39029630978307, 18.82573049530078]}, \"tinfo\": {\"Term\": [\"abi\", \"cok\", \"kanka\", \"sana\", \"benim\", \"seni\", \"bug\\u00fcn\", \"beni\", \"bak\", \"aynen\", \"olsun\", \"sonra\", \"g\\u00fczel\", \"para\", \"kadar\", \"iyiyim\", \"geldi\", \"bana\", \"hadi\", \"yeni\", \"\\u00f6yle\", \"ka\\u00e7\", \"gel\", \"stop\", \"ho\\u015f\", \"sadece\", \"valo\", \"warning\", \"spamming\", \"diyorum\", \"cok\", \"olsun\", \"g\\u00fczel\", \"kadar\", \"\\u00f6yle\", \"zaten\", \"adam\", \"zaman\", \"tamam\", \"geldim\", \"g\\u00fcn\", \"fazla\", \"an\", \"\\u00f6nce\", \"helal\", \"chat\", \"sus\", \"ederim\", \"aga\", \"tabi\", \"te\\u015fekk\\u00fcr\", \"ma\\u00e7\", \"fena\", \"olm\", \"ol\", \"zor\", \"orda\", \"musun\", \"gerek\", \"diyosun\", \"guzel\", \"iyiyim\", \"harika\", \"araba\", \"yemek\", \"ho\\u015fbuldum\", \"hello\", \"imuhamkalp\", \"just\", \"art\\u0131k\", \"yay\\u0131n\", \"kanka\", \"seni\", \"sonra\", \"bana\", \"yeni\", \"gel\", \"ka\\u00e7\", \"ho\\u015f\", \"arada\", \"burda\", \"oyun\", \"demek\", \"yalan\", \"nolur\", \"galiba\", \"senden\", \"degil\", \"bilmiyorum\", \"gitti\", \"baya\", \"dk\", \"harbi\", \"git\", \"20\", \"d\\u00fcn\", \"bizim\", \"buldum\", \"\\u00f6z\", \"geri\", \"ona\", \"4\", \"35\", \"imuhamkalp\", \"abi\", \"sana\", \"beni\", \"bak\", \"aynen\", \"para\", \"geldi\", \"oldu\", \"b\\u00f6yle\", \"biraz\", \"takip\", \"hala\", \"yaz\", \"vs\", \"yayinlar\", \"yap\", \"peki\", \"olarak\", \"olan\", \"anda\", \"olmaz\", \"cocuk\", \"ders\", \"valo\", \"son\", \"yapma\", \"tek\", \"gayet\", \"sende\", \"ye\", \"sabah\", \"bot\", \"imuhamkalp\", \"benim\", \"bug\\u00fcn\", \"hadi\", \"stop\", \"sadece\", \"warning\", \"diyorum\", \"spamming\", \"yaa\", \"yine\", \"dedim\", \"ilk\", \"bunu\", \"tam\", \"abim\", \"dedi\", \"rahat\", \"hemen\", \"allah\", \"devam\", \"erkek\", \"in\", \"misin\", \"k\\u00f6t\\u00fc\", \"i\\u015fte\", \"dimi\", \"ayip\", \"uzun\", \"yoksa\", \"selamlar\", \"caps\", \"to\", \"hediye\", \"imuhamkalp\"], \"Freq\": [637.0, 668.0, 375.0, 306.0, 257.0, 255.0, 213.0, 225.0, 207.0, 202.0, 284.0, 174.0, 242.0, 164.0, 230.0, 277.0, 153.0, 154.0, 131.0, 150.0, 206.0, 146.0, 143.0, 120.0, 137.0, 111.0, 124.0, 107.0, 106.0, 105.0, 667.7809335172055, 283.3437648243485, 241.9969966428969, 229.65662012891335, 206.00800947861654, 170.9997142420607, 167.71011152779357, 146.07061777200363, 123.10626102458119, 126.87858020355641, 124.18126950685505, 121.01789189645507, 107.15751149509848, 109.4797066951136, 113.12180524879346, 103.10261346652881, 94.40167491539007, 94.0972736587375, 89.98842218535977, 88.95318351034133, 102.87722341436064, 109.46695996201892, 89.90027968480389, 76.92939876502453, 71.77384582719449, 76.50934060503425, 73.39192076141023, 64.62915360199874, 79.14428037790938, 80.95931319672862, 70.92038947163337, 268.2720646945913, 76.72254014518822, 92.22595126657211, 87.18746896065937, 109.49709787423043, 78.99840212919807, 268.29041999024764, 82.9416943979663, 92.87362436305254, 85.8901111014574, 375.2261415281605, 255.0324551329602, 173.81153271184655, 154.0090835397259, 149.94453761786787, 143.15115734766104, 145.81956213475732, 136.5551553657557, 118.76128752368706, 116.71365460307126, 99.96604135974052, 84.755153729945, 76.89674765569893, 75.09600909318286, 71.74790582203384, 72.22847362287155, 69.06992210758112, 77.27857018585841, 61.91167575479432, 65.23394211446045, 64.01727490573437, 61.66470032504379, 58.904622209731635, 60.873840135178305, 56.052116176223066, 57.905376695897914, 92.20245169924914, 71.13063250909954, 52.904440734701915, 53.13586911634073, 70.93886687354596, 55.7012255989854, 107.98373451897226, 637.1878344812621, 305.6207375435877, 224.95878357238288, 206.56934901598186, 202.0557245486602, 164.11019986593905, 152.88984447296147, 112.41996220049373, 92.1360516054028, 86.2904472257585, 91.2548907565998, 84.28599514992973, 78.07852708245582, 101.05585832245247, 78.73977129501665, 71.64969532561949, 68.91141603398607, 66.87214018184308, 61.381046097641836, 62.76610628095091, 51.81510358387458, 56.97194020997699, 55.54725439653018, 122.77992933986796, 47.585364360218975, 49.116128026888894, 45.65939867373069, 53.380419335252476, 44.50338042083024, 47.17701187602415, 50.73073513261385, 67.44891193021702, 96.92660069556511, 256.7346961064503, 212.46986112420322, 130.9795457230396, 119.34199384859075, 110.52995101082479, 106.70596957523675, 105.05764092386603, 105.53848290385405, 97.6110907829933, 91.5647658365665, 76.85565740988636, 74.88546468017964, 74.91985452083625, 69.61019224054685, 88.22371211859472, 69.67170163641295, 68.32921661197635, 67.81163585347893, 66.26049760954368, 57.89156476486192, 57.96424278912321, 64.42117994996897, 50.60303904558255, 46.96216054354683, 48.335151963817886, 52.2570687131372, 47.380023179245, 43.361611268366325, 44.72550986578651, 43.993967188922305, 47.868195037183796, 72.83432050028614, 47.91643020285854, 55.842948300332196], \"Total\": [637.0, 668.0, 375.0, 306.0, 257.0, 255.0, 213.0, 225.0, 207.0, 202.0, 284.0, 174.0, 242.0, 164.0, 230.0, 277.0, 153.0, 154.0, 131.0, 150.0, 206.0, 146.0, 143.0, 120.0, 137.0, 111.0, 124.0, 107.0, 106.0, 105.0, 668.4501846247208, 284.02688104628686, 242.65794271407333, 230.31137074394405, 206.67091142089498, 171.65048589840532, 168.37335680299933, 146.7176723624335, 123.74779263775817, 127.55030828904292, 124.84920932834662, 121.68582271521441, 107.82399523751792, 110.17219768988488, 113.89370461801164, 103.81686286764153, 95.06211805268906, 94.7762870407058, 90.6445441766565, 89.62588257923359, 103.69423379137282, 110.33969473390447, 90.62408009689346, 77.60260357491933, 72.42478861294217, 77.21221509666451, 74.07158286553077, 65.27851786460157, 79.94302498660036, 81.78937849754809, 71.69373061929898, 277.16943220609716, 77.77834719348422, 94.85509782307554, 89.3745746666418, 115.80489177030883, 80.27457774111899, 529.0437035051173, 91.12232807946265, 190.53618617343963, 174.46630279794957, 375.92878627429957, 255.7055811895708, 174.50407805955717, 154.6806943103008, 150.63060706955238, 143.8482414822207, 146.55005708692687, 137.3021632200645, 119.45773426381342, 117.40206957446935, 100.67650745985955, 85.4880412737762, 77.59710618506561, 75.7936291297825, 72.43892902815581, 72.926693615628, 69.74679612240227, 78.04061009352392, 62.58796869479095, 65.95502738436248, 64.74081789795339, 62.36291229349076, 59.598767394262126, 61.59761208118968, 56.738841780821524, 58.6276441596695, 93.35528799443613, 72.07774352660701, 53.61516956010644, 53.854981244815825, 72.4456387208929, 58.404946431670915, 529.0437035051173, 637.8916530040447, 306.30093602795233, 225.65123951230024, 207.24216073217295, 202.8140567557168, 164.8436704432437, 153.60189930891684, 113.10098519841478, 92.82264132164748, 86.96790548423245, 91.9770746797741, 85.0108625682654, 78.7911309544593, 102.01766728291994, 79.50699205236384, 72.37058266750745, 69.62814929125298, 67.60768089522239, 62.107320280466524, 63.5320089122975, 52.50066069002506, 57.736286134579665, 56.304285577454394, 124.4771737672504, 48.25944047247043, 49.81189758199575, 46.33222787112849, 54.181539524856035, 45.17615759260974, 47.90285955530679, 51.516541334544094, 69.2801107349281, 529.0437035051173, 257.45916329816845, 213.22149978882325, 131.71146816115015, 120.10220709737209, 111.23600181043005, 107.47206952487856, 105.83470378190354, 106.3204588609303, 98.36855303309206, 92.27878225048029, 77.58341727373815, 75.60109878135432, 75.64730482865285, 70.31323502418999, 89.11957982808042, 70.40767630534582, 69.07436287568363, 68.55366415136649, 67.02408781211568, 58.601188230055456, 58.71426364751479, 65.31022359768427, 51.315207123285305, 47.65699813109742, 49.05898646603959, 53.06150754071262, 48.14988156259312, 44.06774384003393, 45.46400275353855, 44.74221852256093, 48.697850914250665, 78.46739404488562, 49.907093473432056, 529.0437035051173], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.0881, -4.9454, -5.1031, -5.1554, -5.2641, -5.4504, -5.4698, -5.6079, -5.779, -5.7488, -5.7703, -5.7961, -5.9177, -5.8963, -5.8636, -5.9563, -6.0445, -6.0477, -6.0923, -6.1039, -5.9585, -5.8964, -6.0933, -6.2491, -6.3185, -6.2546, -6.2962, -6.4234, -6.2207, -6.1981, -6.3305, -5.0, -6.2518, -6.0678, -6.124, -5.8961, -6.2226, -5.0, -6.1739, -6.0608, -6.139, -4.2681, -4.6542, -5.0376, -5.1586, -5.1853, -5.2317, -5.2132, -5.2789, -5.4185, -5.4359, -5.5908, -5.7558, -5.8531, -5.8768, -5.9224, -5.9157, -5.9605, -5.8482, -6.0699, -6.0176, -6.0364, -6.0739, -6.1197, -6.0868, -6.1693, -6.1368, -5.6716, -5.9311, -6.2271, -6.2227, -5.9338, -6.1756, -5.5136, -3.6842, -4.4189, -4.7253, -4.8106, -4.8327, -5.0407, -5.1115, -5.419, -5.618, -5.6835, -5.6276, -5.707, -5.7835, -5.5256, -5.7751, -5.8694, -5.9084, -5.9384, -6.0241, -6.0018, -6.1935, -6.0987, -6.124, -5.3308, -6.2787, -6.247, -6.32, -6.1638, -6.3457, -6.2873, -6.2147, -5.9299, -5.5673, -4.4198, -4.609, -5.0928, -5.1858, -5.2625, -5.2977, -5.3133, -5.3087, -5.3868, -5.4508, -5.6259, -5.6519, -5.6514, -5.7249, -5.4879, -5.724, -5.7435, -5.7511, -5.7742, -5.9093, -5.908, -5.8024, -6.0438, -6.1185, -6.0897, -6.0116, -6.1096, -6.1982, -6.1673, -6.1838, -6.0994, -5.6796, -6.0984, -5.9453], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0447, 1.0433, 1.043, 1.0429, 1.0425, 1.0419, 1.0418, 1.0413, 1.0405, 1.0405, 1.0404, 1.0402, 1.0395, 1.0394, 1.0389, 1.0388, 1.0388, 1.0386, 1.0385, 1.0382, 1.0378, 1.0378, 1.0377, 1.037, 1.0367, 1.0366, 1.0365, 1.0358, 1.0357, 1.0355, 1.0349, 1.0131, 1.0321, 1.0176, 1.021, 0.9897, 1.0297, 0.3667, 0.9517, 0.3271, 0.3371, 1.4403, 1.4395, 1.4382, 1.4378, 1.4376, 1.4373, 1.4372, 1.4367, 1.4363, 1.4363, 1.4351, 1.4336, 1.4331, 1.4329, 1.4326, 1.4326, 1.4324, 1.4324, 1.4313, 1.4312, 1.4309, 1.4309, 1.4305, 1.4304, 1.43, 1.4298, 1.4298, 1.429, 1.4288, 1.4287, 1.4212, 1.3948, -0.1469, 1.4954, 1.4943, 1.4935, 1.4933, 1.4928, 1.4921, 1.4919, 1.4905, 1.4891, 1.4887, 1.4887, 1.488, 1.4875, 1.4871, 1.4868, 1.4865, 1.4862, 1.4856, 1.4848, 1.4844, 1.4834, 1.4832, 1.483, 1.4828, 1.4825, 1.4825, 1.4819, 1.4816, 1.4815, 1.4813, 1.4812, 1.4698, -0.2006, 1.6671, 1.6664, 1.6644, 1.6636, 1.6636, 1.6628, 1.6626, 1.6626, 1.6622, 1.6622, 1.6605, 1.6604, 1.6603, 1.6599, 1.6598, 1.6594, 1.6591, 1.6591, 1.6585, 1.6578, 1.6571, 1.6562, 1.656, 1.6553, 1.6551, 1.6547, 1.6538, 1.6538, 1.6536, 1.6531, 1.6528, 1.5954, 1.6292, -0.5786]}, \"token.table\": {\"Topic\": [2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 4, 1, 3, 1, 2, 3, 4, 2, 1, 2, 3, 4, 4, 3, 3, 2, 2, 3, 4, 2, 3, 2, 1, 2, 3, 4, 1, 2, 4, 2, 3, 4, 1, 3, 1, 4, 4, 2, 2, 3, 4, 4, 4, 1, 2, 2, 1, 4, 1, 1, 2, 3, 2, 3, 1, 1, 2, 2, 2, 1, 1, 1, 4, 3, 2, 1, 1, 2, 3, 4, 1, 1, 4, 2, 1, 2, 3, 4, 4, 1, 2, 3, 4, 4, 1, 2, 3, 4, 4, 1, 2, 3, 4, 1, 2, 2, 4, 1, 4, 1, 2, 1, 3, 3, 3, 1, 3, 1, 2, 1, 2, 3, 3, 4, 3, 4, 3, 4, 3, 2, 2, 3, 2, 4, 4, 1, 1, 3, 4, 1, 3, 1, 1, 2, 3, 4, 4, 1, 2, 3, 3, 4, 4, 2, 3, 3, 3, 1, 2, 3, 4, 3, 3, 1, 2, 3, 4, 2, 4, 4, 1, 1, 1, 1, 1, 2], \"Freq\": [0.9902981290832835, 0.01712183746576875, 0.9588228980830501, 0.01712183746576875, 0.01712183746576875, 0.013803453425990788, 0.9800451932453459, 0.9986021873779888, 0.9874373304919055, 0.997782566018232, 0.9928893218835063, 0.9847206005251956, 0.9923579604363315, 0.9916261279722492, 0.9699004282468731, 0.010542395959205142, 0.010542395959205142, 0.010542395959205142, 0.9961682324996843, 0.4880962607037005, 0.21518222246077118, 0.18894048801433566, 0.11021528467502914, 0.9761187042360984, 0.9959861916440176, 0.9988315083604735, 0.9955993583211149, 0.9855200214109999, 0.9971139555284174, 0.9982165587261049, 0.9866657873089811, 0.9888705439225747, 0.9892943991070127, 0.014434157067474812, 0.014434157067474812, 0.9670885235208123, 0.9942712165985464, 0.010711766001510262, 0.9854824721389441, 0.9914431210719398, 0.9965752769442082, 0.9911374928580531, 0.9856697800590939, 0.9921316937819343, 0.9872474281968288, 0.9993265247956008, 0.9942097747470348, 0.9924801292049347, 0.9892927537332084, 0.9942911164356515, 0.9945956941939028, 0.9897410232076639, 0.9799947722951868, 0.9921131372595549, 0.9903486429161245, 0.9885571742525543, 0.9869782012175077, 0.9918092693336638, 0.9878349211393879, 0.9943639883438232, 0.9931135290286399, 0.9939407024089878, 0.9781929503071061, 0.9941032196606622, 0.9960814331617975, 0.995685558926319, 0.988203786549753, 0.9885262032153644, 0.989953359432736, 0.9906057233194743, 0.9903236919978017, 0.9931981200929095, 0.9972886001310552, 0.9945982823585289, 0.9881090188038776, 0.994180639098719, 0.9899927522045695, 0.020037231792156922, 0.020037231792156922, 0.020037231792156922, 0.9617871260235322, 0.9921531692993125, 0.9841222741123667, 0.9919236388277657, 0.9977992828883533, 0.9412383046494627, 0.017270427608247024, 0.017270427608247024, 0.008635213804123512, 0.9920490734785118, 0.5065744062813664, 0.2041419249193566, 0.18334969182571842, 0.10585136847670341, 0.9799384610018281, 0.9669175921272626, 0.01443160585264571, 0.010823704389484282, 0.007215802926322855, 0.9784140166292946, 0.91086347056037, 0.032922776044350725, 0.032922776044350725, 0.02194851736290048, 0.9986480444150965, 0.9975293558029847, 0.9962466265939385, 0.9862140261270734, 0.9878584516919747, 0.9938574325047931, 0.995733391723457, 0.9895290786456001, 0.9941347621294091, 0.9821708572280039, 0.9910116589243142, 0.9902654676572153, 0.9922347505475436, 0.9904637259142116, 0.9963845638747146, 0.9841243794900008, 0.9855331447759647, 0.9932803841042134, 0.9948819967368163, 0.990978515188944, 0.9844462861334354, 0.9899732916619981, 0.9978783684545561, 0.999017515154035, 0.9834112266429821, 0.9961006512727731, 0.9872928063829098, 0.9972406500230133, 0.99462404723448, 0.9971113680255361, 0.9969859153697834, 0.9908227573496757, 0.9888271156329552, 0.9930167206032222, 0.9893769759130101, 0.9955451484477676, 0.99395712342161, 0.9928294432106186, 0.9933049913578649, 0.0382324408312058, 0.025488293887470533, 0.012744146943735267, 0.9303227268926745, 0.9757703992310148, 0.008033601420528854, 0.008033601420528854, 0.9881329747250491, 0.990024597601338, 0.9956075143340448, 0.9962533449793851, 0.9923050457108343, 0.9948793742726928, 0.9837007297170464, 0.9936233023124565, 0.49293186489769947, 0.21207533722342883, 0.18341650786891142, 0.10890355154716616, 0.9899591369628065, 0.981152282688586, 0.9734312059609936, 0.01118886443633326, 0.01118886443633326, 0.01118886443633326, 0.9958135528906074, 0.9969789127719136, 0.9897940628753276, 0.9951084804517575, 0.9962104045613345, 0.9972515346645757, 0.9893603130874777, 0.9967537210907798, 0.9850474852031242], \"Term\": [\"20\", \"35\", \"35\", \"35\", \"35\", \"4\", \"4\", \"abi\", \"abim\", \"adam\", \"aga\", \"allah\", \"an\", \"anda\", \"araba\", \"araba\", \"araba\", \"araba\", \"arada\", \"art\\u0131k\", \"art\\u0131k\", \"art\\u0131k\", \"art\\u0131k\", \"ayip\", \"aynen\", \"bak\", \"bana\", \"baya\", \"beni\", \"benim\", \"bilmiyorum\", \"biraz\", \"bizim\", \"bot\", \"bot\", \"bot\", \"bug\\u00fcn\", \"buldum\", \"buldum\", \"bunu\", \"burda\", \"b\\u00f6yle\", \"caps\", \"chat\", \"cocuk\", \"cok\", \"dedi\", \"dedim\", \"degil\", \"demek\", \"ders\", \"devam\", \"dimi\", \"diyorum\", \"diyosun\", \"dk\", \"d\\u00fcn\", \"ederim\", \"erkek\", \"fazla\", \"fena\", \"galiba\", \"gayet\", \"gel\", \"geldi\", \"geldim\", \"gerek\", \"geri\", \"git\", \"gitti\", \"guzel\", \"g\\u00fcn\", \"g\\u00fczel\", \"hadi\", \"hala\", \"harbi\", \"harika\", \"hediye\", \"hediye\", \"hediye\", \"hediye\", \"helal\", \"hello\", \"hemen\", \"ho\\u015f\", \"ho\\u015fbuldum\", \"ho\\u015fbuldum\", \"ho\\u015fbuldum\", \"ho\\u015fbuldum\", \"ilk\", \"imuhamkalp\", \"imuhamkalp\", \"imuhamkalp\", \"imuhamkalp\", \"in\", \"iyiyim\", \"iyiyim\", \"iyiyim\", \"iyiyim\", \"i\\u015fte\", \"just\", \"just\", \"just\", \"just\", \"kadar\", \"kanka\", \"ka\\u00e7\", \"k\\u00f6t\\u00fc\", \"ma\\u00e7\", \"misin\", \"musun\", \"nolur\", \"ol\", \"olan\", \"olarak\", \"oldu\", \"olm\", \"olmaz\", \"olsun\", \"ona\", \"orda\", \"oyun\", \"para\", \"peki\", \"rahat\", \"sabah\", \"sadece\", \"sana\", \"selamlar\", \"sende\", \"senden\", \"seni\", \"son\", \"sonra\", \"spamming\", \"stop\", \"sus\", \"tabi\", \"takip\", \"tam\", \"tamam\", \"tek\", \"te\\u015fekk\\u00fcr\", \"to\", \"to\", \"to\", \"to\", \"uzun\", \"valo\", \"valo\", \"valo\", \"vs\", \"warning\", \"yaa\", \"yalan\", \"yap\", \"yapma\", \"yayinlar\", \"yay\\u0131n\", \"yay\\u0131n\", \"yay\\u0131n\", \"yay\\u0131n\", \"yaz\", \"ye\", \"yemek\", \"yemek\", \"yemek\", \"yemek\", \"yeni\", \"yine\", \"yoksa\", \"zaman\", \"zaten\", \"zor\", \"\\u00f6nce\", \"\\u00f6yle\", \"\\u00f6z\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 1, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1612821714458870406209561259\", ldavis_el1612821714458870406209561259_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1612821714458870406209561259\", ldavis_el1612821714458870406209561259_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1612821714458870406209561259\", ldavis_el1612821714458870406209561259_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.114323  0.002982       1        1  35.142904\n",
       "1     -0.034316 -0.080952       2        1  23.641069\n",
       "0     -0.033838  0.038997       3        1  22.390296\n",
       "3     -0.046169  0.038973       4        1  18.825730, topic_info=            Term        Freq       Total Category  logprob  loglift\n",
       "37           abi  637.000000  637.000000  Default  30.0000  30.0000\n",
       "273          cok  668.000000  668.000000  Default  29.0000  29.0000\n",
       "73         kanka  375.000000  375.000000  Default  28.0000  28.0000\n",
       "586         sana  306.000000  306.000000  Default  27.0000  27.0000\n",
       "494        benim  257.000000  257.000000  Default  26.0000  26.0000\n",
       "...          ...         ...         ...      ...      ...      ...\n",
       "85      selamlar   43.993967   44.742219   Topic4  -6.1838   1.6531\n",
       "8901        caps   47.868195   48.697851   Topic4  -6.0994   1.6528\n",
       "4461          to   72.834321   78.467394   Topic4  -5.6796   1.5954\n",
       "4712      hediye   47.916430   49.907093   Topic4  -6.0984   1.6292\n",
       "353   imuhamkalp   55.842948  529.043704   Topic4  -5.9453  -0.5786\n",
       "\n",
       "[171 rows x 6 columns], token_table=      Topic      Freq   Term\n",
       "term                        \n",
       "431       2  0.990298     20\n",
       "9747      1  0.017122     35\n",
       "9747      2  0.958823     35\n",
       "9747      3  0.017122     35\n",
       "9747      4  0.017122     35\n",
       "...     ...       ...    ...\n",
       "338       1  0.996210  zaten\n",
       "1060      1  0.997252    zor\n",
       "294       1  0.989360   önce\n",
       "307       1  0.996754   öyle\n",
       "1010      2  0.985047     öz\n",
       "\n",
       "[177 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 1, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize the LDA model\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the dominant topic and its percentage contribution in each document\n",
    "\n",
    "def format_topics_sentences(lda_model=None, corpus=corpus, texts=data['final_text']):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(lda_model[corpus]):\n",
    "        row = row_list[0] if lda_model.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = lda_model.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(data['lower'])\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(lda_model=lda_model, corpus=corpus, texts=data['lower'])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35575ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stops,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=50,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot word count and weights of topic keywords\n",
    "\n",
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data['final_text'] for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
