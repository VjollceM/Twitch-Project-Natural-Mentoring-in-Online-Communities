{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a9fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#libraries for k-means\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "\n",
    "data = pd.read_csv(\"preprocessed_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86aaa909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#keep the stopwords\n",
    "\n",
    "nltk.download ('stopwords')\n",
    "#Stop words present in the library\n",
    "stops = nltk.corpus.stopwords.words('turkish')\n",
    "newstops = ['sa','as', 'cisillll', 'pquraen', 'al', 'mis', 'bent', 'imuhamopustuk', 'senin', 'sen', 'onu', 'darkholmee', 'x', 'mu', 'ay', 'naber', 'imuhamiys', 'olur', 'selam', 'insta', 'httpswwwinstagramcomimuhammetyildizhltr', 'haydaaaa', 'hellllooo', 'hellooo', 'hellooooo', 'heee', 'şey', 'haydaaaaaaa', 'haydaaaaaaaa', 'am', 'da', 'daha', 'iyi', 'napalion7', 'bi', 'bosver', 'imuhamalo', 'napalion', 'merhaba', 'httpswwwyoutubecomchannelucstjlfccjuguahupwssiww', 'tutodvrm', 'lul', 'on',  'slm', 'aa', 'şş', 'aaa', 'aaaa', 'aaaaaa', 'aaaaaaa', 'mamiş', 'D', 'çok', 'az', 'neyse', 'falcon2kkalp', 'httpsdiscordggrdj7hxmvgy', ' falcon2kkalp', 'the', 'idil', 'bu', 'httpsdiscordggrdj7hxmvgy', 'neyse', 'ya', 'ne', 'nympea', 'at', 'ays', 'burcinvr', 'san', 'sela', 'değil', 'tutodurm', 'ah', 'be', 'kerema5keremkalp','sö', 'falan', 'bc', 'dc', 'he', 'up', 'aq', 'fizy', '10', 'su', 'nim', 'efe', 'hee', 'hb', 'a', 'gsyi', 'ad', 'ha', 'ner', 'berra', 'hg', 'cafercan3125', 'ayn', 'amk', 'do', 'fa', 'alt','var', 'ba', 'ronaldo', 'ara', 'ada', 'mami', 'bura', 'on', 'fa', 'bb', 'ge', 'ab', 'btw', 'mk', 'pek','Kerem','Keremakdemirr', 'keremakdemirr', 'kere', 'ays055', 'burçin', 'imühamiys', 'mi', 'aç', 'httpswwwyoutubecomchanneluçstjlfçcjuguahupwssiww', 'nympeadansdans','kerem', 'httpsyoutubevsr46l41bru', 'httpswwwinstagramcomkeremakdemirr', 'cafer', 'cüneyt','e', 'i', '5', 'lan', 'la', 'se', 'd', '3', '1', 'bir', 'cafer', 'yay', 'imuhammetyildiz', 'muhamed','deniz', 'Deniz', 'nympea', 'imuhaiys', 'alo', 'evet', 'hayir', 'yok', 'oha']\n",
    "stops.extend(newstops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472a6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    min_df = 5,\n",
    "    max_df = 0.95,\n",
    "    max_features = 4000,\n",
    "    stop_words = stops\n",
    ")\n",
    "tfidf.fit(data.nostopwords)\n",
    "text = tfidf.transform(data.final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb788581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find optimal number of clusters with elbow method\n",
    "\n",
    "def find_optimal_clusters(data, max_k):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "        print('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')\n",
    "    \n",
    "find_optimal_clusters(text, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "329c52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run K-means with defined number of k\n",
    "\n",
    "clusters = MiniBatchKMeans(n_clusters=6, init_size=1024, batch_size=2048, random_state=20).fit_predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319102dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the clusters\n",
    "\n",
    "def plot_tsne_pca(data, labels):\n",
    "    max_label = max(labels)\n",
    "    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)\n",
    "    \n",
    "    pca = PCA(n_components=2).fit_transform(data[max_items,:].toarray())\n",
    "    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].toarray()))\n",
    "    \n",
    "    \n",
    "    idx = np.random.choice(range(pca.shape[0]), size=300, replace=False)\n",
    "    label_subset = labels[max_items]\n",
    "    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n",
    "    \n",
    "    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n",
    "    ax[0].set_title('PCA Cluster Plot')\n",
    "    \n",
    "    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n",
    "    ax[1].set_title('TSNE Cluster Plot')\n",
    "    \n",
    "plot_tsne_pca(text, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a85566bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "gitmek,izlemek,kız,söylemek,abla,etmek,yemek,kanka,yapmak,abi\n",
      "\n",
      "Cluster 1\n",
      "sak,yapmak,etmek,keşke,gün,güzel,aşık,kanka,demek,olmak\n",
      "\n",
      "Cluster 2\n",
      "yan,aynı,gece,para,tabi,noldu,kafa,aynen,chat,baba\n",
      "\n",
      "Cluster 3\n",
      "doğru,ön,sonra,aşk,abla,abi,kanka,kız,yapmak,demek\n",
      "\n",
      "Cluster 4\n",
      "üst,fazla,yiğit,olay,oluyor,hello,bul,almak,şaka,hoş\n",
      "\n",
      "Cluster 5\n",
      "böyle,doğru,allah,kal,işte,herkes,xd,ora,teşekkür,alaka\n"
     ]
    }
   ],
   "source": [
    "def get_top_keywords(data, clusters, labels, n_terms):\n",
    "    df = pd.DataFrame(data.toarray()).groupby(clusters).mean()\n",
    "    \n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n",
    "            \n",
    "get_top_keywords(text, clusters, tfidf.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373a487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
